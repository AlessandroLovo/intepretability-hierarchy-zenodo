run_id = '6'

Running on machine: c4140

Non default parameters:
{
    "reg_c": 0.1,
    "regularization": "gradient"
}
No transfer learning




k_fold_cross_val:
	Models will be trained from scratch
	=============
	fold 0 (1/5)
	=============
	k_fold_cross_val_split:
	k_fold_cross_val_split: completed in 11.1 s
	normalize_X:
		0.0000\% of the data have non zero std below 1e-4
		saving to: ./R6--reg_c__0.1--regularization__gradient/fold_0/X_mean.npy and ./R6--reg_c__0.1--regularization__gradient/fold_0/X_std.npy
	normalize_X: completed in 1 min 3.4 s
	normalize_X:
		loading from: ./R6--reg_c__0.1--regularization__gradient/fold_0/X_mean.npy and ./R6--reg_c__0.1--regularization__gradient/fold_0/X_std.npy
	normalize_X: completed in 2.1 s
	after normalization: X_tr.shape = (50560, 95, 288, 2), X_va.shape = (12640, 95, 288, 2), Y_tr.shape = (50560,), Y_va.shape = (12640,)
	 time_start = 31, time_end = 123, leftmargin = None, rightmargin = None, T = 14
	margin_removal_with_sliding_window:
	margin_removal_with_sliding_window: completed in 0.0 s
	margin_removal_with_sliding_window:
	margin_removal_with_sliding_window: completed in 0.0 s
	margin_removal_with_sliding_window:
	margin_removal_with_sliding_window: completed in 0.0 s
	margin_removal_with_sliding_window:
	margin_removal_with_sliding_window: completed in 0.0 s
	After margin removal: X_tr.shape = (50560, 95, 288, 2), X_va.shape = (12640, 95, 288, 2), Y_tr.shape = (50560,), Y_va.shape = (12640,)
	number of training data: 50560 of which 50844.26017706788 negative and -284.2601770678839 positive
	dense args = [['relu', 'relu', None], [False, False, False], [3e-05, 3e-05, 3e-05]]
	
	Model: "sequential_1"
	_________________________________________________________________
	 Layer (type)                Output Shape              Param #   
	=================================================================
	 sequential (Sequential)     (None, 2)                 55058     
	                                                                 
	 Sigma_Activation (Sigma_Act  (None, 2)                0         
	 ivation)                                                        
	                                                                 
	=================================================================
	Total params: 55,058
	Trainable params: 55,058
	Non-trainable params: 0
	_________________________________________________________________
	
	Using CRPS loss
	train_model:
		scheduler_kwargs = {'lr': 0.0008052, 'epoch_tol': 6, 'warmup': False, 'lr_min': 0.0001811, 'decay': 0.10554}
		Converting training data to tensors
		Converting validation data to tensors
		Training the network on 50560 datapoint and validating on 12640
		
		Learning rate for epoch 1 is 0.0008052000193856657
		
		Learning rate for epoch 2 is 0.0008052000193856657
		
		Learning rate for epoch 3 is 0.0008052000193856657
		
		Learning rate for epoch 4 is 0.0008052000193856657
		
		Learning rate for epoch 5 is 0.0008052000193856657
		
		Learning rate for epoch 6 is 0.0008052000193856657
		
		Learning rate for epoch 7 is 0.0007245499291457236
		
		Learning rate for epoch 8 is 0.0006519779562950134
		
		Learning rate for epoch 9 is 0.0005866748397238553
		
		Learning rate for epoch 10 is 0.0005279126344248652
		
		Learning rate for epoch 11 is 0.00047503606765531003
		
		Learning rate for epoch 12 is 0.00042745578684844077
		
		Learning rate for epoch 13 is 0.00038464111275970936
		
		Learning rate for epoch 14 is 0.00034611488808877766
		
		Learning rate for epoch 15 is 0.00031144748209044337
		
		Learning rate for epoch 16 is 0.00028025239589624107
		
		Learning rate for epoch 17 is 0.0002521818969398737
		
		Learning rate for epoch 18 is 0.00022692300262860954
		
		Learning rate for epoch 19 is 0.00020419404609128833
		
		Learning rate for epoch 20 is 0.00018374164937995374
		
		Learning rate for epoch 21 is 0.00018109999655280262
		
		Learning rate for epoch 22 is 0.00018109999655280262
		
		Learning rate for epoch 23 is 0.00018109999655280262
		
		Learning rate for epoch 24 is 0.00018109999655280262
		
		Learning rate for epoch 25 is 0.00018109999655280262
		
		Learning rate for epoch 26 is 0.00018109999655280262
		
		Learning rate for epoch 27 is 0.00018109999655280262
		
		Learning rate for epoch 28 is 0.00018109999655280262
		
		Learning rate for epoch 29 is 0.00018109999655280262
		
		Learning rate for epoch 30 is 0.00018109999655280262
		              loss  ParametricCrossEntropyLoss  ...  val_CRPS        lr
		epoch-1                                         ...                    
		0        29.733801                    0.245171  ...  0.794080  0.000805
		1         2.162145                    0.155671  ...  0.765562  0.000805
		2         1.044543                    0.145504  ...  0.755626  0.000805
		3         0.864704                    0.143151  ...  0.757699  0.000805
		4         0.809042                    0.143754  ...  0.758088  0.000805
		5         0.786120                    0.143018  ...  0.755892  0.000805
		6         0.774517                    0.143078  ...  0.750058  0.000725
		7         0.767100                    0.141988  ...  0.751507  0.000652
		8         0.763660                    0.142039  ...  0.755044  0.000587
		9         0.760766                    0.142152  ...  0.748767  0.000528
		10        0.759340                    0.141906  ...  0.748239  0.000475
		11        0.757848                    0.141963  ...  0.748306  0.000427
		12        0.756033                    0.141382  ...  0.752675  0.000385
		13        0.755298                    0.141743  ...  0.747975  0.000346
		14        0.753725                    0.141211  ...  0.747874  0.000311
		15        0.753367                    0.141063  ...  0.754621  0.000280
		16        0.752154                    0.141303  ...  0.753791  0.000252
		17        0.751533                    0.140844  ...  0.753880  0.000227
		18        0.751295                    0.140504  ...  0.749231  0.000204
		19        0.750830                    0.140487  ...  0.750089  0.000184
		20        0.750666                    0.141046  ...  0.751240  0.000181
		21        0.750769                    0.140771  ...  0.749902  0.000181
		22        0.750363                    0.140555  ...  0.750831  0.000181
		23        0.749906                    0.140670  ...  0.750117  0.000181
		24        0.750128                    0.141129  ...  0.750976  0.000181
		25        0.750123                    0.141063  ...  0.748971  0.000181
		26        0.750048                    0.140632  ...  0.749320  0.000181
		27        0.749804                    0.141039  ...  0.749144  0.000181
		28        0.749367                    0.141168  ...  0.749127  0.000181
		29        0.749528                    0.141069  ...  0.752246  0.000181
		
		[30 rows x 9 columns]
		score = 0.1390303671360016
	train_model: completed in 2 min 35.1 s
	RAM memory: 6.369e+11
	=============
	fold 1 (2/5)
	=============
	k_fold_cross_val_split:
	k_fold_cross_val_split: completed in 1 min 47.6 s
	normalize_X:
		0.0000\% of the data have non zero std below 1e-4
		saving to: ./R6--reg_c__0.1--regularization__gradient/fold_1/X_mean.npy and ./R6--reg_c__0.1--regularization__gradient/fold_1/X_std.npy
	normalize_X: completed in 3 min 26.9 s
	normalize_X:
		loading from: ./R6--reg_c__0.1--regularization__gradient/fold_1/X_mean.npy and ./R6--reg_c__0.1--regularization__gradient/fold_1/X_std.npy
	normalize_X: completed in 2.3 s
	after normalization: X_tr.shape = (50560, 95, 288, 2), X_va.shape = (12640, 95, 288, 2), Y_tr.shape = (50560,), Y_va.shape = (12640,)
	 time_start = 31, time_end = 123, leftmargin = None, rightmargin = None, T = 14
	margin_removal_with_sliding_window:
	margin_removal_with_sliding_window: completed in 0.0 s
	margin_removal_with_sliding_window:
	margin_removal_with_sliding_window: completed in 0.0 s
	margin_removal_with_sliding_window:
	margin_removal_with_sliding_window: completed in 0.0 s
	margin_removal_with_sliding_window:
	margin_removal_with_sliding_window: completed in 0.0 s
	After margin removal: X_tr.shape = (50560, 95, 288, 2), X_va.shape = (12640, 95, 288, 2), Y_tr.shape = (50560,), Y_va.shape = (12640,)
	number of training data: 50560 of which 50776.057928270864 negative and -216.05792827086134 positive
	dense args = [['relu', 'relu', None], [False, False, False], [3e-05, 3e-05, 3e-05]]
	
	Model: "sequential_1"
	_________________________________________________________________
	 Layer (type)                Output Shape              Param #   
	=================================================================
	 sequential (Sequential)     (None, 2)                 55058     
	                                                                 
	 Sigma_Activation (Sigma_Act  (None, 2)                0         
	 ivation)                                                        
	                                                                 
	=================================================================
	Total params: 55,058
	Trainable params: 55,058
	Non-trainable params: 0
	_________________________________________________________________
	
	Using CRPS loss
	train_model:
		scheduler_kwargs = {'lr': 0.0008052, 'epoch_tol': 6, 'warmup': False, 'lr_min': 0.0001811, 'decay': 0.10554}
		Converting training data to tensors
		Converting validation data to tensors
		Training the network on 50560 datapoint and validating on 12640
		
		Learning rate for epoch 1 is 0.0008052000193856657
		
		Learning rate for epoch 2 is 0.0008052000193856657
		
		Learning rate for epoch 3 is 0.0008052000193856657
		
		Learning rate for epoch 4 is 0.0008052000193856657
		
		Learning rate for epoch 5 is 0.0008052000193856657
		
		Learning rate for epoch 6 is 0.0008052000193856657
		
		Learning rate for epoch 7 is 0.0007245499291457236
		
		Learning rate for epoch 8 is 0.0006519779562950134
		
		Learning rate for epoch 9 is 0.0005866748397238553
		
		Learning rate for epoch 10 is 0.0005279126344248652
		
		Learning rate for epoch 11 is 0.00047503606765531003
		
		Learning rate for epoch 12 is 0.00042745578684844077
		
		Learning rate for epoch 13 is 0.00038464111275970936
		
		Learning rate for epoch 14 is 0.00034611488808877766
		
		Learning rate for epoch 15 is 0.00031144748209044337
		
		Learning rate for epoch 16 is 0.00028025239589624107
		
		Learning rate for epoch 17 is 0.0002521818969398737
		
		Learning rate for epoch 18 is 0.00022692300262860954
		
		Learning rate for epoch 19 is 0.00020419404609128833
		
		Learning rate for epoch 20 is 0.00018374164937995374
		
		Learning rate for epoch 21 is 0.00018109999655280262
		
		Learning rate for epoch 22 is 0.00018109999655280262
		
		Learning rate for epoch 23 is 0.00018109999655280262
		
		Learning rate for epoch 24 is 0.00018109999655280262
		
		Learning rate for epoch 25 is 0.00018109999655280262
		
		Learning rate for epoch 26 is 0.00018109999655280262
		
		Learning rate for epoch 27 is 0.00018109999655280262
		
		Learning rate for epoch 28 is 0.00018109999655280262
		              loss  ParametricCrossEntropyLoss  ...  val_CRPS        lr
		epoch-1                                         ...                    
		0        30.196920                    0.306174  ...  0.787225  0.000805
		1         2.175406                    0.155553  ...  0.762766  0.000805
		2         1.048032                    0.150219  ...  0.750987  0.000805
		3         0.869641                    0.146823  ...  0.755565  0.000805
		4         0.813978                    0.144649  ...  0.753909  0.000805
		5         0.790551                    0.143720  ...  0.750102  0.000805
		6         0.777714                    0.142973  ...  0.752511  0.000725
		7         0.770554                    0.143229  ...  0.747772  0.000652
		8         0.766400                    0.143114  ...  0.747832  0.000587
		9         0.763092                    0.143037  ...  0.746177  0.000528
		10        0.760182                    0.143482  ...  0.748705  0.000475
		11        0.758614                    0.142731  ...  0.747887  0.000427
		12        0.757231                    0.142620  ...  0.745878  0.000385
		13        0.756124                    0.142319  ...  0.748598  0.000346
		14        0.755527                    0.142755  ...  0.752654  0.000311
		15        0.754643                    0.141705  ...  0.746242  0.000280
		16        0.753546                    0.142420  ...  0.750883  0.000252
		17        0.753326                    0.142495  ...  0.747094  0.000227
		18        0.752243                    0.141884  ...  0.747567  0.000204
		19        0.751741                    0.141932  ...  0.747812  0.000184
		20        0.751446                    0.142025  ...  0.747230  0.000181
		21        0.751343                    0.141677  ...  0.748823  0.000181
		22        0.751747                    0.142340  ...  0.747334  0.000181
		23        0.751627                    0.142474  ...  0.747845  0.000181
		24        0.751325                    0.141925  ...  0.746539  0.000181
		25        0.751014                    0.142337  ...  0.746927  0.000181
		26        0.751042                    0.141665  ...  0.748396  0.000181
		27        0.750949                    0.142049  ...  0.749773  0.000181
		
		[28 rows x 9 columns]
		score = 0.13663777709007263
	train_model: completed in 2 min 15.1 s
	RAM memory: 6.471e+11
	=============
	fold 2 (3/5)
	=============
	k_fold_cross_val_split:
	k_fold_cross_val_split: completed in 21.1 s
	normalize_X:
		0.0000\% of the data have non zero std below 1e-4
		saving to: ./R6--reg_c__0.1--regularization__gradient/fold_2/X_mean.npy and ./R6--reg_c__0.1--regularization__gradient/fold_2/X_std.npy
	normalize_X: completed in 1 min 49.5 s
	normalize_X:
		loading from: ./R6--reg_c__0.1--regularization__gradient/fold_2/X_mean.npy and ./R6--reg_c__0.1--regularization__gradient/fold_2/X_std.npy
	normalize_X: completed in 2.1 s
	after normalization: X_tr.shape = (50560, 95, 288, 2), X_va.shape = (12640, 95, 288, 2), Y_tr.shape = (50560,), Y_va.shape = (12640,)
	 time_start = 31, time_end = 123, leftmargin = None, rightmargin = None, T = 14
	margin_removal_with_sliding_window:
	margin_removal_with_sliding_window: completed in 0.0 s
	margin_removal_with_sliding_window:
	margin_removal_with_sliding_window: completed in 0.0 s
	margin_removal_with_sliding_window:
	margin_removal_with_sliding_window: completed in 0.0 s
	margin_removal_with_sliding_window:
	margin_removal_with_sliding_window: completed in 0.0 s
	After margin removal: X_tr.shape = (50560, 95, 288, 2), X_va.shape = (12640, 95, 288, 2), Y_tr.shape = (50560,), Y_va.shape = (12640,)
	number of training data: 50560 of which 51998.273299213346 negative and -1438.2732992133472 positive
	dense args = [['relu', 'relu', None], [False, False, False], [3e-05, 3e-05, 3e-05]]
	
	Model: "sequential_1"
	_________________________________________________________________
	 Layer (type)                Output Shape              Param #   
	=================================================================
	 sequential (Sequential)     (None, 2)                 55058     
	                                                                 
	 Sigma_Activation (Sigma_Act  (None, 2)                0         
	 ivation)                                                        
	                                                                 
	=================================================================
	Total params: 55,058
	Trainable params: 55,058
	Non-trainable params: 0
	_________________________________________________________________
	
	Using CRPS loss
	train_model:
		scheduler_kwargs = {'lr': 0.0008052, 'epoch_tol': 6, 'warmup': False, 'lr_min': 0.0001811, 'decay': 0.10554}
		Converting training data to tensors
		Converting validation data to tensors
		Training the network on 50560 datapoint and validating on 12640
		
		Learning rate for epoch 1 is 0.0008052000193856657
		
		Learning rate for epoch 2 is 0.0008052000193856657
		
		Learning rate for epoch 3 is 0.0008052000193856657
		
		Learning rate for epoch 4 is 0.0008052000193856657
		
		Learning rate for epoch 5 is 0.0008052000193856657
		
		Learning rate for epoch 6 is 0.0008052000193856657
		
		Learning rate for epoch 7 is 0.0007245499291457236
		
		Learning rate for epoch 8 is 0.0006519779562950134
		
		Learning rate for epoch 9 is 0.0005866748397238553
		
		Learning rate for epoch 10 is 0.0005279126344248652
		
		Learning rate for epoch 11 is 0.00047503606765531003
		
		Learning rate for epoch 12 is 0.00042745578684844077
		
		Learning rate for epoch 13 is 0.00038464111275970936
		
		Learning rate for epoch 14 is 0.00034611488808877766
		
		Learning rate for epoch 15 is 0.00031144748209044337
		
		Learning rate for epoch 16 is 0.00028025239589624107
		
		Learning rate for epoch 17 is 0.0002521818969398737
		
		Learning rate for epoch 18 is 0.00022692300262860954
		
		Learning rate for epoch 19 is 0.00020419404609128833
		
		Learning rate for epoch 20 is 0.00018374164937995374
		
		Learning rate for epoch 21 is 0.00018109999655280262
		
		Learning rate for epoch 22 is 0.00018109999655280262
		
		Learning rate for epoch 23 is 0.00018109999655280262
		
		Learning rate for epoch 24 is 0.00018109999655280262
		
		Learning rate for epoch 25 is 0.00018109999655280262
		
		Learning rate for epoch 26 is 0.00018109999655280262
		
		Learning rate for epoch 27 is 0.00018109999655280262
		
		Learning rate for epoch 28 is 0.00018109999655280262
		
		Learning rate for epoch 29 is 0.00018109999655280262
		
		Learning rate for epoch 30 is 0.00018109999655280262
		
		Learning rate for epoch 31 is 0.00018109999655280262
		
		Learning rate for epoch 32 is 0.00018109999655280262
		
		Learning rate for epoch 33 is 0.00018109999655280262
		
		Learning rate for epoch 34 is 0.00018109999655280262
		
		Learning rate for epoch 35 is 0.00018109999655280262
		
		Learning rate for epoch 36 is 0.00018109999655280262
		
		Learning rate for epoch 37 is 0.00018109999655280262
		
		Learning rate for epoch 38 is 0.00018109999655280262
		
		Learning rate for epoch 39 is 0.00018109999655280262
		
		Learning rate for epoch 40 is 0.00018109999655280262
		
		Learning rate for epoch 41 is 0.00018109999655280262
		
		Learning rate for epoch 42 is 0.00018109999655280262
		              loss  ParametricCrossEntropyLoss  ...  val_CRPS        lr
		epoch-1                                         ...                    
		0        30.018215                    0.317933  ...  0.803626  0.000805
		1         2.241192                    0.154427  ...  0.775590  0.000805
		2         1.067177                    0.147353  ...  0.764485  0.000805
		3         0.872859                    0.143897  ...  0.764331  0.000805
		4         0.813509                    0.142923  ...  0.759989  0.000805
		5         0.788231                    0.141828  ...  0.762196  0.000805
		6         0.776454                    0.142472  ...  0.761794  0.000725
		7         0.768382                    0.142028  ...  0.762015  0.000652
		8         0.764102                    0.142386  ...  0.757634  0.000587
		9         0.761753                    0.141977  ...  0.759828  0.000528
		10        0.759151                    0.142168  ...  0.759327  0.000475
		11        0.757619                    0.141614  ...  0.754738  0.000427
		12        0.755964                    0.141823  ...  0.757177  0.000385
		13        0.754659                    0.141960  ...  0.754594  0.000346
		14        0.753977                    0.141358  ...  0.754662  0.000311
		15        0.753309                    0.141280  ...  0.753196  0.000280
		16        0.752278                    0.141135  ...  0.754957  0.000252
		17        0.751904                    0.141331  ...  0.753863  0.000227
		18        0.751074                    0.141153  ...  0.754916  0.000204
		19        0.750622                    0.140850  ...  0.754276  0.000184
		20        0.750616                    0.141132  ...  0.757974  0.000181
		21        0.750283                    0.140908  ...  0.755849  0.000181
		22        0.749472                    0.141142  ...  0.755548  0.000181
		23        0.749326                    0.141284  ...  0.758378  0.000181
		24        0.749743                    0.141436  ...  0.755776  0.000181
		25        0.749264                    0.140970  ...  0.758549  0.000181
		26        0.749334                    0.141098  ...  0.753096  0.000181
		27        0.748997                    0.140881  ...  0.754617  0.000181
		28        0.749045                    0.141335  ...  0.753992  0.000181
		29        0.748669                    0.141138  ...  0.757317  0.000181
		30        0.748682                    0.141108  ...  0.760612  0.000181
		31        0.749055                    0.141793  ...  0.755483  0.000181
		32        0.748639                    0.141093  ...  0.753935  0.000181
		33        0.748760                    0.141269  ...  0.755337  0.000181
		34        0.749023                    0.141226  ...  0.755209  0.000181
		35        0.748921                    0.141011  ...  0.754013  0.000181
		36        0.748533                    0.141249  ...  0.757387  0.000181
		37        0.748496                    0.141354  ...  0.754242  0.000181
		38        0.749080                    0.141490  ...  0.754000  0.000181
		39        0.748560                    0.141275  ...  0.756085  0.000181
		40        0.748710                    0.141206  ...  0.756448  0.000181
		41        0.748256                    0.141282  ...  0.758359  0.000181
		
		[42 rows x 9 columns]
		score = 0.14168775081634521
	train_model: completed in 3 min 15.7 s
	RAM memory: 6.484e+11
	=============
	fold 3 (4/5)
	=============
	k_fold_cross_val_split:
	k_fold_cross_val_split: completed in 4.5 s
	normalize_X:
		0.0000\% of the data have non zero std below 1e-4
		saving to: ./R6--reg_c__0.1--regularization__gradient/fold_3/X_mean.npy and ./R6--reg_c__0.1--regularization__gradient/fold_3/X_std.npy
	normalize_X: completed in 1 min 38.0 s
	normalize_X:
		loading from: ./R6--reg_c__0.1--regularization__gradient/fold_3/X_mean.npy and ./R6--reg_c__0.1--regularization__gradient/fold_3/X_std.npy
	normalize_X: completed in 2.1 s
	after normalization: X_tr.shape = (50560, 95, 288, 2), X_va.shape = (12640, 95, 288, 2), Y_tr.shape = (50560,), Y_va.shape = (12640,)
	 time_start = 31, time_end = 123, leftmargin = None, rightmargin = None, T = 14
	margin_removal_with_sliding_window:
	margin_removal_with_sliding_window: completed in 0.0 s
	margin_removal_with_sliding_window:
	margin_removal_with_sliding_window: completed in 0.0 s
	margin_removal_with_sliding_window:
	margin_removal_with_sliding_window: completed in 0.0 s
	margin_removal_with_sliding_window:
	margin_removal_with_sliding_window: completed in 0.0 s
	After margin removal: X_tr.shape = (50560, 95, 288, 2), X_va.shape = (12640, 95, 288, 2), Y_tr.shape = (50560,), Y_va.shape = (12640,)
	number of training data: 50560 of which 51196.11055805184 negative and -636.1105580518446 positive
	dense args = [['relu', 'relu', None], [False, False, False], [3e-05, 3e-05, 3e-05]]
	
	Model: "sequential_1"
	_________________________________________________________________
	 Layer (type)                Output Shape              Param #   
	=================================================================
	 sequential (Sequential)     (None, 2)                 55058     
	                                                                 
	 Sigma_Activation (Sigma_Act  (None, 2)                0         
	 ivation)                                                        
	                                                                 
	=================================================================
	Total params: 55,058
	Trainable params: 55,058
	Non-trainable params: 0
	_________________________________________________________________
	
	Using CRPS loss
	train_model:
		scheduler_kwargs = {'lr': 0.0008052, 'epoch_tol': 6, 'warmup': False, 'lr_min': 0.0001811, 'decay': 0.10554}
		Converting training data to tensors
		Converting validation data to tensors
		Training the network on 50560 datapoint and validating on 12640
		
		Learning rate for epoch 1 is 0.0008052000193856657
		
		Learning rate for epoch 2 is 0.0008052000193856657
		
		Learning rate for epoch 3 is 0.0008052000193856657
		
		Learning rate for epoch 4 is 0.0008052000193856657
		
		Learning rate for epoch 5 is 0.0008052000193856657
		
		Learning rate for epoch 6 is 0.0008052000193856657
		
		Learning rate for epoch 7 is 0.0007245499291457236
		
		Learning rate for epoch 8 is 0.0006519779562950134
		
		Learning rate for epoch 9 is 0.0005866748397238553
		
		Learning rate for epoch 10 is 0.0005279126344248652
		
		Learning rate for epoch 11 is 0.00047503606765531003
		
		Learning rate for epoch 12 is 0.00042745578684844077
		
		Learning rate for epoch 13 is 0.00038464111275970936
		
		Learning rate for epoch 14 is 0.00034611488808877766
		
		Learning rate for epoch 15 is 0.00031144748209044337
		
		Learning rate for epoch 16 is 0.00028025239589624107
		
		Learning rate for epoch 17 is 0.0002521818969398737
		
		Learning rate for epoch 18 is 0.00022692300262860954
		
		Learning rate for epoch 19 is 0.00020419404609128833
		
		Learning rate for epoch 20 is 0.00018374164937995374
		
		Learning rate for epoch 21 is 0.00018109999655280262
		
		Learning rate for epoch 22 is 0.00018109999655280262
		
		Learning rate for epoch 23 is 0.00018109999655280262
		
		Learning rate for epoch 24 is 0.00018109999655280262
		
		Learning rate for epoch 25 is 0.00018109999655280262
		
		Learning rate for epoch 26 is 0.00018109999655280262
		
		Learning rate for epoch 27 is 0.00018109999655280262
		
		Learning rate for epoch 28 is 0.00018109999655280262
		
		Learning rate for epoch 29 is 0.00018109999655280262
		
		Learning rate for epoch 30 is 0.00018109999655280262
		
		Learning rate for epoch 31 is 0.00018109999655280262
		
		Learning rate for epoch 32 is 0.00018109999655280262
		
		Learning rate for epoch 33 is 0.00018109999655280262
		
		Learning rate for epoch 34 is 0.00018109999655280262
		
		Learning rate for epoch 35 is 0.00018109999655280262
		
		Learning rate for epoch 36 is 0.00018109999655280262
		
		Learning rate for epoch 37 is 0.00018109999655280262
		
		Learning rate for epoch 38 is 0.00018109999655280262
		
		Learning rate for epoch 39 is 0.00018109999655280262
		
		Learning rate for epoch 40 is 0.00018109999655280262
		
		Learning rate for epoch 41 is 0.00018109999655280262
		
		Learning rate for epoch 42 is 0.00018109999655280262
		              loss  ParametricCrossEntropyLoss  ...  val_CRPS        lr
		epoch-1                                         ...                    
		0        29.845194                    0.371866  ...  0.862710  0.000805
		1         2.133489                    0.158759  ...  0.764588  0.000805
		2         1.039641                    0.148094  ...  0.756117  0.000805
		3         0.864229                    0.144635  ...  0.750531  0.000805
		4         0.808443                    0.142461  ...  0.749913  0.000805
		5         0.784915                    0.142828  ...  0.747669  0.000805
		6         0.774038                    0.141806  ...  0.750973  0.000725
		7         0.767217                    0.142169  ...  0.749573  0.000652
		8         0.764741                    0.142664  ...  0.747780  0.000587
		9         0.761542                    0.141728  ...  0.748380  0.000528
		10        0.759631                    0.141786  ...  0.753765  0.000475
		11        0.758240                    0.141566  ...  0.749516  0.000427
		12        0.756683                    0.141622  ...  0.747404  0.000385
		13        0.754945                    0.141989  ...  0.746394  0.000346
		14        0.755053                    0.141527  ...  0.745078  0.000311
		15        0.753952                    0.141325  ...  0.748137  0.000280
		16        0.753458                    0.141320  ...  0.746676  0.000252
		17        0.753001                    0.141519  ...  0.743797  0.000227
		18        0.751851                    0.141534  ...  0.745885  0.000204
		19        0.751221                    0.140774  ...  0.744924  0.000184
		20        0.751463                    0.141458  ...  0.743968  0.000181
		21        0.750712                    0.141166  ...  0.746630  0.000181
		22        0.750840                    0.141101  ...  0.746893  0.000181
		23        0.750929                    0.141385  ...  0.744398  0.000181
		24        0.750346                    0.141171  ...  0.745154  0.000181
		25        0.751162                    0.141302  ...  0.748098  0.000181
		26        0.750631                    0.141259  ...  0.743388  0.000181
		27        0.750121                    0.141311  ...  0.746485  0.000181
		28        0.750287                    0.140982  ...  0.746096  0.000181
		29        0.749919                    0.141448  ...  0.745218  0.000181
		30        0.749911                    0.140926  ...  0.744688  0.000181
		31        0.750012                    0.141569  ...  0.744552  0.000181
		32        0.750109                    0.141402  ...  0.745688  0.000181
		33        0.749677                    0.141736  ...  0.743800  0.000181
		34        0.749691                    0.141398  ...  0.747977  0.000181
		35        0.750061                    0.141332  ...  0.747642  0.000181
		36        0.749840                    0.141274  ...  0.748207  0.000181
		37        0.750046                    0.141267  ...  0.745744  0.000181
		38        0.750003                    0.141552  ...  0.746466  0.000181
		39        0.749955                    0.141369  ...  0.747994  0.000181
		40        0.749920                    0.141740  ...  0.744510  0.000181
		41        0.750107                    0.141266  ...  0.747506  0.000181
		
		[42 rows x 9 columns]
		score = 0.13783186674118042
	train_model: completed in 3 min 17.0 s
	RAM memory: 6.484e+11
	=============
	fold 4 (5/5)
	=============
	k_fold_cross_val_split:
	k_fold_cross_val_split: completed in 1.2 s
	normalize_X:
		0.0000\% of the data have non zero std below 1e-4
		saving to: ./R6--reg_c__0.1--regularization__gradient/fold_4/X_mean.npy and ./R6--reg_c__0.1--regularization__gradient/fold_4/X_std.npy
	normalize_X: completed in 53.6 s
	normalize_X:
		loading from: ./R6--reg_c__0.1--regularization__gradient/fold_4/X_mean.npy and ./R6--reg_c__0.1--regularization__gradient/fold_4/X_std.npy
	normalize_X: completed in 2.1 s
	after normalization: X_tr.shape = (50560, 95, 288, 2), X_va.shape = (12640, 95, 288, 2), Y_tr.shape = (50560,), Y_va.shape = (12640,)
	 time_start = 31, time_end = 123, leftmargin = None, rightmargin = None, T = 14
	margin_removal_with_sliding_window:
	margin_removal_with_sliding_window: completed in 0.0 s
	margin_removal_with_sliding_window:
	margin_removal_with_sliding_window: completed in 0.0 s
	margin_removal_with_sliding_window:
	margin_removal_with_sliding_window: completed in 0.0 s
	margin_removal_with_sliding_window:
	margin_removal_with_sliding_window: completed in 0.0 s
	After margin removal: X_tr.shape = (50560, 95, 288, 2), X_va.shape = (12640, 95, 288, 2), Y_tr.shape = (50560,), Y_va.shape = (12640,)
	number of training data: 50560 of which 51071.648143603656 negative and -511.64814360365324 positive
	dense args = [['relu', 'relu', None], [False, False, False], [3e-05, 3e-05, 3e-05]]
	
	Model: "sequential_1"
	_________________________________________________________________
	 Layer (type)                Output Shape              Param #   
	=================================================================
	 sequential (Sequential)     (None, 2)                 55058     
	                                                                 
	 Sigma_Activation (Sigma_Act  (None, 2)                0         
	 ivation)                                                        
	                                                                 
	=================================================================
	Total params: 55,058
	Trainable params: 55,058
	Non-trainable params: 0
	_________________________________________________________________
	
	Using CRPS loss
	train_model:
		scheduler_kwargs = {'lr': 0.0008052, 'epoch_tol': 6, 'warmup': False, 'lr_min': 0.0001811, 'decay': 0.10554}
		Converting training data to tensors
		Converting validation data to tensors
		Training the network on 50560 datapoint and validating on 12640
		
		Learning rate for epoch 1 is 0.0008052000193856657
		
		Learning rate for epoch 2 is 0.0008052000193856657
		
		Learning rate for epoch 3 is 0.0008052000193856657
		
		Learning rate for epoch 4 is 0.0008052000193856657
		
		Learning rate for epoch 5 is 0.0008052000193856657
		
		Learning rate for epoch 6 is 0.0008052000193856657
		
		Learning rate for epoch 7 is 0.0007245499291457236
		
		Learning rate for epoch 8 is 0.0006519779562950134
		
		Learning rate for epoch 9 is 0.0005866748397238553
		
		Learning rate for epoch 10 is 0.0005279126344248652
		
		Learning rate for epoch 11 is 0.00047503606765531003
		
		Learning rate for epoch 12 is 0.00042745578684844077
		
		Learning rate for epoch 13 is 0.00038464111275970936
		
		Learning rate for epoch 14 is 0.00034611488808877766
		
		Learning rate for epoch 15 is 0.00031144748209044337
		
		Learning rate for epoch 16 is 0.00028025239589624107
		
		Learning rate for epoch 17 is 0.0002521818969398737
		
		Learning rate for epoch 18 is 0.00022692300262860954
		
		Learning rate for epoch 19 is 0.00020419404609128833
		
		Learning rate for epoch 20 is 0.00018374164937995374
		
		Learning rate for epoch 21 is 0.00018109999655280262
		
		Learning rate for epoch 22 is 0.00018109999655280262
		
		Learning rate for epoch 23 is 0.00018109999655280262
		
		Learning rate for epoch 24 is 0.00018109999655280262
		
		Learning rate for epoch 25 is 0.00018109999655280262
		
		Learning rate for epoch 26 is 0.00018109999655280262
		
		Learning rate for epoch 27 is 0.00018109999655280262
		              loss  ParametricCrossEntropyLoss  ...  val_CRPS        lr
		epoch-1                                         ...                    
		0        28.439926                    0.207631  ...  0.802512  0.000805
		1         2.085252                    0.151494  ...  0.784682  0.000805
		2         1.019621                    0.144053  ...  0.774262  0.000805
		3         0.852464                    0.142183  ...  0.767959  0.000805
		4         0.800188                    0.142050  ...  0.768579  0.000805
		5         0.780309                    0.141778  ...  0.769595  0.000805
		6         0.769818                    0.141261  ...  0.768850  0.000725
		7         0.764066                    0.141066  ...  0.768117  0.000652
		8         0.759874                    0.141087  ...  0.767489  0.000587
		9         0.757053                    0.140665  ...  0.769211  0.000528
		10        0.754793                    0.140868  ...  0.767065  0.000475
		11        0.753818                    0.140621  ...  0.762640  0.000427
		12        0.752146                    0.140282  ...  0.766030  0.000385
		13        0.751196                    0.140673  ...  0.766015  0.000346
		14        0.750481                    0.140113  ...  0.763016  0.000311
		15        0.749226                    0.140340  ...  0.766003  0.000280
		16        0.748971                    0.140158  ...  0.766339  0.000252
		17        0.748250                    0.140044  ...  0.765354  0.000227
		18        0.747377                    0.140213  ...  0.766706  0.000204
		19        0.746497                    0.140280  ...  0.766869  0.000184
		20        0.746770                    0.139583  ...  0.765966  0.000181
		21        0.746032                    0.140090  ...  0.770863  0.000181
		22        0.746483                    0.139819  ...  0.766930  0.000181
		23        0.746266                    0.139996  ...  0.763485  0.000181
		24        0.746566                    0.140153  ...  0.767260  0.000181
		25        0.746095                    0.140246  ...  0.767940  0.000181
		26        0.746284                    0.140082  ...  0.765372  0.000181
		
		[27 rows x 9 columns]
		score = 0.14601579308509827
	train_model: completed in 2 min 7.9 s
	RAM memory: 6.491e+11
	
	Final scores:
		fold 0: 0.1390303671360016
		fold 1: 0.13663777709007263
		fold 2: 0.14168775081634521
		fold 3: 0.13783186674118042
		fold 4: 0.14601579308509827
	Average score: 0.1402+/-0.0033
k_fold_cross_val: completed in 25 min 6.5 s
